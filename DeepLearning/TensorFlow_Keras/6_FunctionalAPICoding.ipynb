{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining different layers**\n",
    "* Input layer of given shape\n",
    "* Hidden layer with given number of neurons that are used to read the complex patterns\n",
    "* Fully connected layers that connect input and output layers\n",
    "* Output layer give the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Define the Input Layer**\n",
    "\n",
    "You will define the input shape for your model. For simplicity, let's assume you are working with a dataset where each input is a vector of length 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 20), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\")\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(20, ))\n",
    "print(input_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Add Hidden Layers**\n",
    "\n",
    "Next, you will add a couple of hidden layers to your model. Hidden layers help the model learn complex patterns in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 21:05:13.376043: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-06-11 21:05:13.376073: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-06-11 21:05:13.376078: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-06-11 21:05:13.376407: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-06-11 21:05:13.376696: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_1 = Dense(64, activation='relu')(input_layer)\n",
    "hidden_layer_2 = Dense(64, activation='relu')(hidden_layer_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "`Dense(64, activation='relu')` creates a dense (fully connected) layer with 64 units and ReLU activation function. \n",
    "\n",
    "Each hidden layer takes the output of the previous layer as its input. \n",
    "\n",
    "**Step 3: Define the Output Layer**\n",
    "\n",
    "Finally, you will define the output layer. Suppose you are working on a binary classification problem, so the output layer will have one unit with a sigmoid activation function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "`Dense(1, activation='sigmoid')` creates a dense layer with 1 unit and a sigmoid activation function, suitable for binary classification. \n",
    "\n",
    "**Step 4: Create the Model**\n",
    "\n",
    "Now, you will create the model by specifying the input and output layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1344      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5569 (21.75 KB)\n",
      "Trainable params: 5569 (21.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "`Model(inputs=input_layer, outputs=output_layer)` creates a Keras model that connects the input layer to the output layer through the hidden layers. \n",
    "\n",
    "`model.summary()` provides a summary of the model, showing the layers, their shapes, and the number of parameters. This helps you interpret the model architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Compile the Model**\n",
    "\n",
    "Before training the model, you need to compile it. You will specify the loss function, optimizer, and evaluation metrics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "`optimizer='adam'` specifies the Adam optimizer, a popular choice for training neural networks. \n",
    "\n",
    "`loss='binary_crossentropy'` specifies the loss function for binary classification problems. \n",
    "\n",
    "`metrics=['accuracy']` instructs Keras to evaluate the model using accuracy during training. \n",
    "\n",
    "**Step 6: Train the Model**\n",
    "\n",
    "You can now train the model using training data. For this example, let's assume `X_train` is your training input data and `y_train` is the corresponding label. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets create the training datasets first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 21:06:21.398323: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 22ms/step - loss: 0.7216 - accuracy: 0.5230\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6897 - accuracy: 0.5330\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5510\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6888 - accuracy: 0.5450\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6904 - accuracy: 0.5460\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6905 - accuracy: 0.5450\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6886 - accuracy: 0.5390\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6923 - accuracy: 0.5360\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6922 - accuracy: 0.5460\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6913 - accuracy: 0.5230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x305a01840>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.random.randn(1000, 20)\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "`X_train` and `y_train` are placeholders for your actual training data. \n",
    "\n",
    "`model.fit` trains the model for a specified number of epochs and batch size. \n",
    "\n",
    "**Step 7: Evaluate the Model**\n",
    "\n",
    "After training, you can evaluate the model on test data to see how well it performs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6999 - accuracy: 0.5250\n",
      "Test Loss: 0.6999\n",
      "Test Accuracy: 0.5250\n"
     ]
    }
   ],
   "source": [
    "X_test = np.random.randn(200, 20)\n",
    "y_test = np.random.randint(2, size=(200, 1))\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "`model.evaluate` computes the loss and accuracy of the model on test data. \n",
    "\n",
    "`X_test` and `y_test` are placeholders for your actual test data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dropout and Batch Normalization**\n",
    "\n",
    "Before we proceed with the practice exercise, let's briefly discuss two important techniques often used to improve the performance of neural networks: **Dropout Layers** and **Batch Normalization**.\n",
    "\n",
    "#### Dropout Layers\n",
    "\n",
    "Dropout is a regularization technique that helps prevent overfitting in neural networks. During training, Dropout randomly sets a fraction of input units to zero at each update cycle. This prevents the model from becoming overly reliant on any specific neurons, which encourages the network to learn more robust features that generalize better to unseen data.\n",
    "\n",
    "**Key points:**\n",
    "- Dropout is only applied during training, not during inference.\n",
    "- The dropout rate is a hyperparameter that determines the fraction of neurons to drop.\n",
    "\n",
    "\n",
    "#### Batch Normalization\n",
    "\n",
    "Batch Normalization is a technique used to improve the training stability and speed of neural networks. It normalizes the output of a previous layer by re-centering and re-scaling the data, which helps in stabilizing the learning process. By reducing the internal covariate shift (the changes in the distribution of layer inputs), batch normalization allows the model to use higher learning rates, which often speeds up convergence.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- Batch normalization works by normalizing the inputs to each layer to have a mean of zero and a variance of one.\n",
    "- It is applied during both training and inference, although its behavior varies slightly between the two phases.\n",
    "- Batch normalization layers also introduce two learnable parameters that allow the model to scale and - shift the normalized output, which helps in restoring the model's representational power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of adding a Dropout layer in Keras:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                1344      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5569 (21.75 KB)\n",
      "Trainable params: 5569 (21.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Add Input layer first\n",
    "input_layer = Input(shape=(20, ))\n",
    "\n",
    "# Add a hidden layer\n",
    "hidden_layer_1 = Dense(64, activation='relu')(input_layer)\n",
    "\n",
    "# Add a Dropout layer\n",
    "dropout_layer = Dropout(rate=0.5)(hidden_layer_1)\n",
    "\n",
    "# Add another hidden layer after Dropout\n",
    "hidden_layer_2 = Dense(64, activation='relu')(dropout_layer)\n",
    "\n",
    "# Define the output layer\n",
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer_2)\n",
    "\n",
    "# Create a model\n",
    "model = Model(inputs = input_layer, outputs = output_layer)\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit on train sets and evaluate with test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.8186 - accuracy: 0.5110\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7460 - accuracy: 0.5430\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7206 - accuracy: 0.5410\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7129 - accuracy: 0.4840\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.7081 - accuracy: 0.5200\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7003 - accuracy: 0.5160\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6922 - accuracy: 0.5280\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5390\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6962 - accuracy: 0.5340\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x30cc2ff70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7105 - accuracy: 0.4950\n",
      "Test Loss: 0.7105\n",
      "Test accuracy: 0.4950\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of adding Batch Normalization in Keras:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                1344      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 64)                256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5825 (22.75 KB)\n",
      "Trainable params: 5697 (22.25 KB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define input layer\n",
    "input_layer = Input(shape=(20, ))\n",
    "\n",
    "# Define hidden layer\n",
    "hidden_layer_1 = Dense(64, activation='relu')(input_layer)\n",
    "\n",
    "# Add a batch normalization layer\n",
    "normalization = BatchNormalization()(hidden_layer_1)\n",
    "\n",
    "# Add another hidden layer\n",
    "hidden_layer_2 = Dense(64, activation='relu')(normalization)\n",
    "\n",
    "# Add output layer\n",
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer_2)\n",
    "\n",
    "# Define model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compiling the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit on train sets and evaluate with test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 9ms/step - loss: 0.7071 - accuracy: 0.5410\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6979 - accuracy: 0.5120\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6975 - accuracy: 0.5160\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6941 - accuracy: 0.5340\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6970 - accuracy: 0.5220\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6994 - accuracy: 0.5130\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.7033 - accuracy: 0.5210\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6929 - accuracy: 0.5400\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6994 - accuracy: 0.5200\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3118d5300>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7080 - accuracy: 0.5200\n",
      "Test Loss: 0.7080\n",
      "Test accuracy: 0.5200\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding `Dropout` layer after each hidden layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                1344      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5569 (21.75 KB)\n",
      "Trainable params: 5569 (21.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 8ms/step - loss: 0.9922 - accuracy: 0.4920\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.8955 - accuracy: 0.5160\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.8774 - accuracy: 0.5060\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.8271 - accuracy: 0.5150\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.8350 - accuracy: 0.4940\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.8180 - accuracy: 0.5110\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.8060 - accuracy: 0.5370\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.8114 - accuracy: 0.5100\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.7786 - accuracy: 0.5070\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7721 - accuracy: 0.5270\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.7005 - accuracy: 0.5400\n",
      "Test loss: 0.7005350589752197\n",
      "Test accuracy: 0.5400000214576721\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout, Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(20,))\n",
    "\n",
    "# Add hidden layers with dropout\n",
    "hidden_layer1 = Dense(64, activation='relu')(input_layer)\n",
    "dropout1 = Dropout(0.5)(hidden_layer1)\n",
    "hidden_layer2 = Dense(64, activation='relu')(dropout1)\n",
    "dropout2 = Dropout(0.5)(hidden_layer2)\n",
    "\n",
    "# Define the output layer\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing the activation function from *relu* to *tanh* function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                1344      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5569 (21.75 KB)\n",
      "Trainable params: 5569 (21.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 8ms/step - loss: 0.7804 - accuracy: 0.5240\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.7736 - accuracy: 0.4990\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.7502 - accuracy: 0.5260\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.7358 - accuracy: 0.5270\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7361 - accuracy: 0.5190\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7347 - accuracy: 0.5140\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7302 - accuracy: 0.5090\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7190 - accuracy: 0.5320\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7315 - accuracy: 0.5150\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7194 - accuracy: 0.5340\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6972 - accuracy: 0.5000\n",
      "Test loss: 0.6971697211265564\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout, Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(20,))\n",
    "\n",
    "# Add hidden layers with dropout\n",
    "hidden_layer1 = Dense(64, activation='tanh')(input_layer)\n",
    "dropout1 = Dropout(0.5)(hidden_layer1)\n",
    "hidden_layer2 = Dense(64, activation='tanh')(dropout1)\n",
    "dropout2 = Dropout(0.5)(hidden_layer2)\n",
    "\n",
    "# Define the output layer\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding `BatchNormalization` after each hidden layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 64)                1344      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6081 (23.75 KB)\n",
      "Trainable params: 5825 (22.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 12ms/step - loss: 0.7691 - accuracy: 0.5210\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.6982 - accuracy: 0.5290\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.6994 - accuracy: 0.5470\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.7002 - accuracy: 0.5340\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5410\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.6995 - accuracy: 0.5030\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.6865 - accuracy: 0.5570\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.6936 - accuracy: 0.5340\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5470\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.6967 - accuracy: 0.5400\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.7093 - accuracy: 0.4950\n",
      "Test loss: 0.7093\n",
      "Test accuracy: 0.4950\n"
     ]
    }
   ],
   "source": [
    "## Write your code herefrom tensorflow.keras.layers import Dropout, Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(20,))\n",
    "\n",
    "# Add hidden layers with dropout\n",
    "hidden_layer1 = Dense(64, activation='relu')(input_layer)\n",
    "batch_norm1 = BatchNormalization()(hidden_layer1)\n",
    "hidden_layer2 = Dense(64, activation='relu')(batch_norm1)\n",
    "batch_norm2 = BatchNormalization()(hidden_layer2)\n",
    "\n",
    "# Define the output layer\n",
    "output_layer = Dense(1, activation='sigmoid')(batch_norm2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
